#!/usr/bin/env python3
"""
DSL ì…ë ¥ë¶€í„° ì‹¤í–‰ ê³„íš ìƒì„±ê¹Œì§€ - TTL íŒŒì¼ ë¡œë“œ ë²„ì „
í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬: pip install rdflib
í•„ìš” íŒŒì¼: 
  - execution-ontology.ttl
  - domain-ontology.ttl  
  - bridge-ontology.ttl
  - test_data.json (ì„ íƒì‚¬í•­)
"""

from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, XSD
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
import os,json

class OntologyBasedExecutionPlanner:
    def __init__(self, 
                 exec_ttl: str = "execution-ontology.ttl",
                 domain_ttl: str = "domain-ontology.ttl", 
                 bridge_ttl: str = "bridge-ontology.ttl",
                 test_data_json: str = "test_data.json"):
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
        self.EXEC = Namespace("http://example.org/execution-ontology#")
        self.PROD = Namespace("http://example.org/production-domain#")
        self.BRIDGE = Namespace("http://example.org/bridge-ontology#")
        
        # 3ê°œì˜ ê·¸ë˜í”„ ì´ˆê¸°í™”
        self.exec_graph = Graph()
        self.domain_graph = Graph()
        self.bridge_graph = Graph()
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©
        self._bind_namespaces()
        
        # ì‹¤í–‰ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # TTL íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤í–‰ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
        self.exec_ttl = os.path.join(self.base_dir, exec_ttl)
        self.domain_ttl = os.path.join(self.base_dir, domain_ttl)
        self.bridge_ttl = os.path.join(self.base_dir, bridge_ttl)
        self.test_data_json = os.path.join(self.base_dir, test_data_json)
        
        # ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        self._load_ontologies()
        self._load_action_metadata()  # â† ìƒˆë¡œ ì¶”ê°€
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        self._load_test_data()
        
    def _bind_namespaces(self):
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°”ì¸ë”©"""
        for graph in [self.exec_graph, self.domain_graph, self.bridge_graph]:
            graph.bind("exec", self.EXEC)
            graph.bind("prod", self.PROD)
            graph.bind("bridge", self.BRIDGE)
            graph.bind("rdf", RDF)
            graph.bind("rdfs", RDFS)
            graph.bind("owl", OWL)
            graph.bind("xsd", XSD)
    
    def _load_ontologies(self):
        """3ê°œì˜ ì˜¨í†¨ë¡œì§€ TTL íŒŒì¼ ë¡œë“œ"""
        print("ğŸ“š ì˜¨í†¨ë¡œì§€ TTL íŒŒì¼ ë¡œë“œ ì¤‘...")
        print(f"   ê¸°ì¤€ ë””ë ‰í† ë¦¬: {self.base_dir}")
        
        # ì‹¤í–‰ ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        try:
            self.exec_graph.parse(self.exec_ttl, format="turtle")
            print(f"âœ… ì‹¤í–‰ ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì„±ê³µ: {os.path.basename(self.exec_ttl)}")
            print(f"   - {len(self.exec_graph)} íŠ¸ë¦¬í”Œ")
        except Exception as e:
            print(f"âŒ ì‹¤í–‰ ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   íŒŒì¼ ê²½ë¡œ: {self.exec_ttl}")
            raise
        
        # ë„ë©”ì¸ ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        try:
            self.domain_graph.parse(self.domain_ttl, format="turtle")
            print(f"âœ… ë„ë©”ì¸ ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì„±ê³µ: {os.path.basename(self.domain_ttl)}")
            print(f"   - {len(self.domain_graph)} íŠ¸ë¦¬í”Œ")
        except Exception as e:
            print(f"âŒ ë„ë©”ì¸ ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   íŒŒì¼ ê²½ë¡œ: {self.domain_ttl}")
            raise
        
        # ë¸Œë¦¬ì§€ ì˜¨í†¨ë¡œì§€ ë¡œë“œ
        try:
            self.bridge_graph.parse(self.bridge_ttl, format="turtle")
            print(f"âœ… ë¸Œë¦¬ì§€ ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì„±ê³µ: {os.path.basename(self.bridge_ttl)}")
            print(f"   - {len(self.bridge_graph)} íŠ¸ë¦¬í”Œ")
        except Exception as e:
            print(f"âŒ ë¸Œë¦¬ì§€ ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print(f"   íŒŒì¼ ê²½ë¡œ: {self.bridge_ttl}")
            raise
        
        # ì˜¨í†¨ë¡œì§€ ë‚´ìš© ê²€ì¦
        self._validate_ontologies()
    
    def _validate_ontologies(self):
        """ë¡œë“œëœ ì˜¨í†¨ë¡œì§€ ê²€ì¦"""
        print("\nğŸ” ì˜¨í†¨ë¡œì§€ ê²€ì¦ ì¤‘...")
        
        # Goal í™•ì¸ - rdf:type ì‚¬ìš©
        query = """
        PREFIX exec: <http://example.org/execution-ontology#>
        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        SELECT ?goal ?label WHERE {
            ?goal rdf:type ?goalType .
            ?goalType rdfs:subClassOf* exec:Goal .
            OPTIONAL { ?goal rdfs:label ?label }
        }
        """
        goals = list(self.exec_graph.query(query))
        print(f"   - Goal ê°œìˆ˜: {len(goals)}")
        for goal, label in goals:
            print(f"     â€¢ {label if label else str(goal).split('#')[-1]}")
        
        # Action í™•ì¸ - rdf:type ì‚¬ìš©
        query = """
        PREFIX exec: <http://example.org/execution-ontology#>
        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
        SELECT DISTINCT ?action WHERE {
            ?action rdf:type ?actionType .
            ?actionType rdfs:subClassOf* exec:Action .
        }
        """
        actions = list(self.exec_graph.query(query))
        print(f"   - Action ê°œìˆ˜: {len(actions)}")
        
        # Machine í™•ì¸
        query = """
        PREFIX prod: <http://example.org/production-domain#>
        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
        SELECT DISTINCT ?machine WHERE {
            ?machine rdf:type prod:Machine .
        }
        """
        machines = list(self.domain_graph.query(query))
        print(f"   - Machine ê°œìˆ˜: {len(machines)}")
    
    def _load_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
        print(f"   ê¸°ì¤€ ë””ë ‰í† ë¦¬: {self.base_dir}")
        
        # JSON íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        if os.path.exists(self.test_data_json):
            try:
                with open(self.test_data_json, "r", encoding="utf-8") as f:
                    test_data = json.load(f)
                self._load_json_to_graph(test_data)
                print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {os.path.basename(self.test_data_json)}")
            except Exception as e:
                print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("   ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
                self._create_sample_test_data()
        else:
            print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {os.path.basename(self.test_data_json)}")
            print("   ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            self._create_sample_test_data()
    
    def _load_json_to_graph(self, test_data: Dict):
        """JSON í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€"""
        # Machine ë°ì´í„° ì¶”ê°€
        if "machines" in test_data:
            for machine in test_data["machines"]:
                machine_uri = self.PROD[machine["id"]]
                self.domain_graph.add((machine_uri, RDF.type, self.PROD.Machine))
                self.domain_graph.add((machine_uri, self.PROD.hasStatus, Literal(machine["status"])))
                self.domain_graph.add((machine_uri, self.PROD.hasCapability, 
                                     self.PROD[machine["capability"].title() + "Capability"]))
                if "aasEndpoint" in machine:
                    endpoint = URIRef(machine["aasEndpoint"])
                    self.domain_graph.add((machine_uri, self.PROD.hasAASEndpoint, endpoint))
        
        # Job ë°ì´í„° ì¶”ê°€
        if "jobs" in test_data:
            for job in test_data["jobs"]:
                job_uri = self.PROD[job["id"]]
                self.domain_graph.add((job_uri, RDF.type, self.PROD.Job))
                self.domain_graph.add((job_uri, self.PROD.hasStatus, Literal(job["status"])))
                
                if job.get("requiresCooling"):
                    self.domain_graph.add((job_uri, self.PROD.requiresCooling, 
                                         Literal(True, datatype=XSD.boolean)))
                if job.get("requiresHeating"):
                    self.domain_graph.add((job_uri, self.PROD.requiresHeating, 
                                         Literal(True, datatype=XSD.boolean)))
                
                if "startTime" in job:
                    self.domain_graph.add((job_uri, self.PROD.hasStartTime, 
                                         Literal(job["startTime"], datatype=XSD.dateTime)))
                
                if job.get("executedOn"):
                    if isinstance(job["executedOn"], list):
                        machine_id = job["executedOn"][0] if job["executedOn"] else None
                    else:
                        machine_id = job["executedOn"]
                    
                    if machine_id:
                        self.domain_graph.add((job_uri, self.PROD.executedOn, self.PROD[machine_id]))
                
                if job.get("forProduct"):
                    self.domain_graph.add((job_uri, self.PROD.forProduct, self.PROD[job["forProduct"]]))
                
                if job.get("currentOperationIndex"):
                    self.domain_graph.add((job_uri, self.PROD.currentOperationIndex, 
                                         Literal(job["currentOperationIndex"], datatype=XSD.integer)))
    
    def _create_sample_test_data(self):
        """ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        # ê¸°ë³¸ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€
        sample_data = {
            "machines": [
                {"id": "CoolingMachine-01", "capability": "cooling", "status": "Active"},
                {"id": "CoolingMachine-02", "capability": "cooling", "status": "Active"}
            ],
            "jobs": [
                {
                    "id": "Job-101",
                    "status": "Failed",
                    "requiresCooling": True,
                    "startTime": "2025-07-17T10:00:00",
                    "executedOn": "CoolingMachine-01"
                },
                {
                    "id": "Job-102",
                    "status": "Success",
                    "requiresCooling": True,
                    "startTime": "2025-07-17T11:00:00",
                    "executedOn": "CoolingMachine-02"
                }
            ]
        }
        self._load_json_to_graph(sample_data)
    
    def process_dsl(self, dsl_input: Dict[str, Any]) -> Dict[str, Any]:
        """DSL ì…ë ¥ì„ ì²˜ë¦¬í•˜ì—¬ ì‹¤í–‰ ê³„íš ìƒì„±"""
        print(f"\nğŸš€ DSL ì²˜ë¦¬ ì‹œì‘: {dsl_input['goal']}")
        print(f"ì…ë ¥: {json.dumps(dsl_input, indent=2)}")
        
        # Step 1: Goal ë§¤í•‘ ì°¾ê¸°
        goal_name = dsl_input["goal"]
        execution_goal = self._find_execution_goal(goal_name)
        
        if not execution_goal:
            return {"error": f"Unknown goal: {goal_name}"}
        
        print(f"\nâœ… Step 1: Goal ë§¤í•‘ ì™„ë£Œ")
        print(f"   {goal_name} â†’ {execution_goal}")
        
        # Step 2: í•„ìš”í•œ Actionë“¤ ì°¾ê¸°
        actions = self._find_required_actions(execution_goal)
        print(f"\nâœ… Step 2: í•„ìš”í•œ Action ì‹ë³„")
        for action in actions:
            print(f"   - {action['name']} (order: {action['order']})")
        
        # Step 3: íŒŒë¼ë¯¸í„° ë§¤í•‘
        parameter_mappings = self._map_parameters(goal_name, dsl_input)
        print(f"\nâœ… Step 3: íŒŒë¼ë¯¸í„° ë§¤í•‘")
        for key, value in parameter_mappings.items():
            print(f"   {key}: {value}")
        
        # Step 4: ì‹¤í–‰ ê³„íš ìƒì„±
        execution_plan = self._generate_execution_plan(
            goal_name,
            dsl_input,
            execution_goal,
            actions,
            parameter_mappings
        )
        
        print(f"\nâœ… Step 4: ì‹¤í–‰ ê³„íš ìƒì„± ì™„ë£Œ")
        
        return execution_plan
    
    def _find_execution_goal(self, goal_name: str) -> Optional[URIRef]:
        """Goal ì´ë¦„ìœ¼ë¡œ ì‹¤í–‰ Goal ì°¾ê¸°"""
        # Bridge ì˜¨í†¨ë¡œì§€ì—ì„œ ë§¤í•‘ ì°¾ê¸°
        query = f"""
        PREFIX bridge: <{self.BRIDGE}>
        PREFIX exec: <{self.EXEC}>
        
        SELECT ?execGoal
        WHERE {{
            ?mapping bridge:mapsGoal "{goal_name}" ;
                     bridge:toExecutionGoal ?execGoal .
        }}
        """
        
        results = list(self.bridge_graph.query(query))
        if results:
            return results[0][0]
        
        # ì§ì ‘ ë§¤í•‘ ì‹œë„
        query2 = f"""
        PREFIX exec: <{self.EXEC}>
        PREFIX rdfs: <{RDFS}>
        
        SELECT ?goal
        WHERE {{
            ?goal rdfs:label "{goal_name}" .
        }}
        """
        
        results2 = list(self.exec_graph.query(query2))
        if results2:
            return results2[0][0]
        
        return None
    
    def _find_required_actions(self, execution_goal: URIRef) -> List[Dict]:
        """Goalì— í•„ìš”í•œ Actionë“¤ ì°¾ê¸°"""
        query = f"""
        PREFIX exec: <{self.EXEC}>
        PREFIX rdfs: <{RDFS}>
        
        SELECT ?action ?type ?order
        WHERE {{
            <{execution_goal}> exec:requiresAction ?action .
            OPTIONAL {{ ?action exec:actionType ?type }}
            OPTIONAL {{ ?action exec:executionOrder ?order }}
        }}
        ORDER BY ?order
        """
        
        results = self.exec_graph.query(query)
        
        actions = []
        for action, action_type, order in results:
            actions.append({
                "uri": action,
                "name": str(action).split('#')[-1],
                "type": str(action_type).split('#')[-1] if action_type else "Unknown",
                "order": int(order) if order else 0
            })
        
        return sorted(actions, key=lambda x: x["order"])
    
    def _map_parameters(self, goal_name: str, dsl_input: Dict) -> Dict:
        """DSL íŒŒë¼ë¯¸í„°ë¥¼ ë„ë©”ì¸ ê°œë…ìœ¼ë¡œ ë§¤í•‘"""
        mappings = {}
        
        # Bridge ì˜¨í†¨ë¡œì§€ì—ì„œ ì•”ë¬µì  í•„í„° ì°¾ê¸°
        query = f"""
        PREFIX bridge: <{self.BRIDGE}>
        PREFIX prod: <{self.PROD}>
        
        SELECT ?property ?value
        WHERE {{
            ?mapping bridge:mapsGoal "{goal_name}" .
            ?mapping bridge:hasImplicitFilter ?filter .
            ?filter bridge:filterProperty ?property .
            ?filter bridge:filterValue ?value .
        }}
        """
        
        results = self.bridge_graph.query(query)
        
        for prop, value in results:
            prop_name = str(prop).split('#')[-1]
            mappings[prop_name] = value.value if hasattr(value, 'value') else str(value)
        
        # DSL íŒŒë¼ë¯¸í„° ì¶”ê°€
        if "date" in dsl_input:
            mappings["dateFilter"] = dsl_input["date"]
        
        # Goalë³„ ì¶”ê°€ ë§¤í•‘
        if goal_name == "detect_anomaly_for_product":
            mappings["product_id"] = dsl_input.get("product_id")
            mappings["date_range"] = dsl_input.get("date_range")
            mappings["target_machine"] = dsl_input.get("target_machine")
        elif goal_name == "predict_first_completion_time":
            mappings["product_id"] = dsl_input.get("product_id")
            mappings["quantity"] = dsl_input.get("quantity")
        elif goal_name == "track_product_position":
            mappings["product_id"] = dsl_input.get("product_id")
            mappings["statusFilter"] = "Processing"
        
        return mappings
    
    def _generate_execution_plan(self, goal_name: str, dsl_input: Dict, 
                            execution_goal: URIRef, actions: List[Dict],
                            parameter_mappings: Dict) -> Dict:
        """ì‹¤í–‰ ê³„íš ìƒì„±"""
        
        execution_steps = []
        
        for idx, action in enumerate(actions):
            action_name = action["name"]
            metadata = self.action_metadata.get(action_name, {})
            
            step = {
                "stepId": idx + 1,
                "action": action_name,
                "type": action["type"],
                "parameters": {}
            }
            
            # ì˜¨í†¨ë¡œì§€ì—ì„œ ë¡œë“œí•œ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
            if metadata.get('outputName'):
                step["output"] = metadata['outputName']
            
            # Action íƒ€ì…ë³„ ì²˜ë¦¬
            if "QueryBuilder" in action_name or action_name.startswith("Build"):
                # ì¿¼ë¦¬ ë¹Œë” ì²˜ë¦¬
                if metadata.get('queryTemplate'):
                    step["generatedQuery"] = self._process_query_template(
                        metadata['queryTemplate'], 
                        parameter_mappings, 
                        dsl_input
                    )
                
            elif ("Execute" in action_name and "Query" in action_name) or action_name == "ExecuteProductTrace":
                # ì¿¼ë¦¬ ì‹¤í–‰ ì²˜ë¦¬
                step["input"] = metadata.get('inputMapping', '')
                step["endpoint"] = metadata.get('endpointTemplate', '')
                
                # ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
                if action_name == "ExecuteJobQuery":
                    sample_results = self._execute_job_query(parameter_mappings)
                elif action_name == "ExecuteProductTrace":
                    sample_results = self._execute_product_trace_query(dsl_input)
                elif action_name == "ExecuteLocationQuery":
                    sample_results = self._execute_location_query(dsl_input)
                else:
                    sample_results = []
                
                step["expectedResultCount"] = len(sample_results)
                step["sampleResults"] = sample_results[:3]
                
            elif action_name == "FetchSensorData":
                # AAS ì»¤ë„¥í„° ì²˜ë¦¬
                endpoint = metadata.get('endpointTemplate', '')
                step["endpoint"] = endpoint.format(**dsl_input)
    
            elif action_name == "FetchJobTemplate":
                # FetchJobTemplate íŠ¹ë³„ ì²˜ë¦¬
                step["input"] = metadata.get('inputMapping', '')
                step["endpoint"] = metadata.get('endpointTemplate', '')
                
                sample_results = self._execute_job_template_query(dsl_input)
                step["expectedResultCount"] = len(sample_results)
                step["sampleResults"] = sample_results[:3]
                
            elif action_name == "FetchMachineSchedule":
                # ë¨¸ì‹  ìŠ¤ì¼€ì¤„ ì²˜ë¦¬
                if metadata.get('queryTemplate'):
                    step["generatedQuery"] = metadata['queryTemplate']
                    
            elif "RunAnomalyDetection" in action_name or "RunSimulation" in action_name:
                # Docker ì‹¤í–‰ ì²˜ë¦¬
                step["model"] = metadata.get('dockerImage', '')
                
                if metadata.get('inputMapping'):
                    input_mapping = metadata['inputMapping']
                    # JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ê³  í…œí”Œë¦¿ ë³€ìˆ˜ ì¹˜í™˜
                    import json
                    try:
                        input_dict = json.loads(input_mapping)
                        # í…œí”Œë¦¿ ë³€ìˆ˜ ì¹˜í™˜
                        for key, value in input_dict.items():
                            if isinstance(value, str):
                                value = value.replace("%%PRODUCT_ID%%", dsl_input.get('product_id', ''))
                                value = value.replace("%%QUANTITY%%", str(dsl_input.get('quantity', 1)))
                            input_dict[key] = value
                        step["input"] = input_dict
                    except:
                        processed_input = input_mapping
                        processed_input = processed_input.replace("%%PRODUCT_ID%%", dsl_input.get('product_id', ''))
                        processed_input = processed_input.replace("%%QUANTITY%%", str(dsl_input.get('quantity', 1)))
                        try:
                            step["input"] = json.loads(processed_input)
                        except:
                            step["input"] = processed_input
                        
            elif "Enrich" in action_name:
                # ë°ì´í„° ë³´ê°• ì²˜ë¦¬
                step["input"] = metadata.get('inputMapping', '')
                if metadata.get('enrichmentType'):
                    enrichment_types = metadata['enrichmentType'].split(',')
                    step["enrichments"] = []
                    for etype in enrichment_types:
                        if etype == "fetchMachineDetails":
                            step["enrichments"].append({"type": etype, "source": "AAS"})
                        elif etype == "parseErrorLogs":
                            step["enrichments"].append({"type": etype, "source": "Domain"})
                        elif etype == "addOperationDetails":
                            step["enrichments"].append({"type": etype, "source": "Domain"})
                        elif etype == "calculateProgress":
                            step["enrichments"].append({"type": etype, "source": "Compute"})
            
            execution_steps.append(step)
        
        # ìµœì¢… ì‹¤í–‰ ê³„íš (ê¸°ì¡´ê³¼ ë™ì¼)
        execution_plan = {
            "executionId": f"exec-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
            "goal": goal_name,
            "inputParameters": dsl_input,
            "parameterMappings": parameter_mappings,
            "executionSteps": execution_steps,
            "expectedOutput": {
                "type": self._get_result_type(goal_name),
                "format": "JSON"
            },
            "estimatedDuration": f"~{len(actions) * 0.5 + 1} seconds"
        }
        
        return execution_plan
    
    def _build_job_query(self, params: Dict) -> str:
        """Job ì¡°íšŒ ì¿¼ë¦¬ ìƒì„±"""
        filters = []
        if params.get("requiresCooling"):
            filters.append("?job prod:requiresCooling true")
        if params.get("hasStatus"):
            filters.append(f'?job prod:hasStatus "{params["hasStatus"]}"')
        if params.get("dateFilter"):
            filters.append(f'FILTER(STRSTARTS(STR(?startTime), "{params["dateFilter"]}"))')
        
        query = f"""
        PREFIX prod: <{self.PROD}>
        PREFIX rdf: <{RDF}>
        
        SELECT ?job ?machine ?startTime ?status
        WHERE {{
            ?job rdf:type prod:Job .
            ?job prod:hasStartTime ?startTime .
            ?job prod:hasStatus ?status .
            {' . '.join(filters)}
            OPTIONAL {{ ?job prod:executedOn ?machine }}
        }}
        ORDER BY DESC(?startTime)
        LIMIT 100
        """
        
        return query.strip()
    
    def _build_product_trace_query(self, dsl_input: Dict) -> str:
        """ì œí’ˆ ì¶”ì  ì¿¼ë¦¬ ìƒì„±"""
        product_id = dsl_input.get("product_id", "Unknown")
        date_range = dsl_input.get("date_range", {})
        
        query = f"""
        PREFIX prod: <{self.PROD}>
        PREFIX rdf: <{RDF}>
        
        SELECT ?job ?machine ?startTime ?status
        WHERE {{
            ?job rdf:type prod:Job .
            ?job prod:forProduct prod:{product_id} .
            ?job prod:executedOn ?machine .
            ?job prod:hasStartTime ?startTime .
            ?job prod:hasStatus ?status .
        """
        
        if date_range:
            # ë‚ ì§œì— ì‹œê°„ ì •ë³´ ì¶”ê°€
            from_date = f"{date_range.get('from')}T00:00:00"
            to_date = f"{date_range.get('to')}T23:59:59"
            
            query += f"""
            FILTER(?startTime >= "{from_date}"^^xsd:dateTime && 
                ?startTime <= "{to_date}"^^xsd:dateTime)
            """
        
        query += """
        }
        ORDER BY ?startTime
        """
        
        return query.strip()
    
    def _build_job_template_query(self, dsl_input: Dict) -> str:
        """Job í…œí”Œë¦¿ ì¡°íšŒ ì¿¼ë¦¬"""
        product_id = dsl_input.get("product_id", "Unknown")
        
        query = f"""
        PREFIX prod: <{self.PROD}>
        PREFIX rdf: <{RDF}>
        
        SELECT ?operation ?machine ?duration
        WHERE {{
            prod:{product_id} prod:hasJobTemplate ?template .
            ?template prod:hasOperation ?operation .
            ?operation prod:canBeExecutedOn ?machine ;
                      prod:hasDuration ?duration .
        }}
        """
        
        return query.strip()
    
    def _build_location_query(self, dsl_input: Dict) -> str:
        """ìœ„ì¹˜ ì¡°íšŒ ì¿¼ë¦¬"""
        product_id = dsl_input.get("product_id", "Unknown")
        
        query = f"""
        PREFIX prod: <{self.PROD}>
        PREFIX rdf: <{RDF}>
        
        SELECT ?job ?status ?machine ?opIndex
        WHERE {{
            ?job rdf:type prod:Job .
            ?job prod:forProduct prod:{product_id} .
            ?job prod:hasStatus "Processing" .
            ?job prod:executedOn ?machine .
            OPTIONAL {{ ?job prod:currentOperationIndex ?opIndex }}
        }}
        """
        
        return query.strip()
    
    def _execute_job_query(self, params: Dict) -> List[Dict]:
        """Job ì¿¼ë¦¬ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œ SPARQL ì¿¼ë¦¬ ì‹¤í–‰
        query = self._build_job_query(params)
        
        # ì¿¼ë¦¬ë¥¼ ë„ë©”ì¸ ê·¸ë˜í”„ì—ì„œ ì‹¤í–‰
        results = []
        try:
            query_result = self.domain_graph.query(query)
            for row in query_result:
                results.append({
                    "job": str(row.job).split('#')[-1] if row.job else None,
                    "status": str(row.status) if hasattr(row, 'status') else None,
                    "machine": str(row.machine).split('#')[-1] if hasattr(row, 'machine') and row.machine else None,
                    "startTime": str(row.startTime) if hasattr(row, 'startTime') else None
                })
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _execute_product_trace_query(self, dsl_input: Dict) -> List[Dict]:
        """ì œí’ˆ ì¶”ì  ì¿¼ë¦¬ ì‹¤í–‰"""
        query = self._build_product_trace_query(dsl_input)
        
        results = []
        try:
            query_result = self.domain_graph.query(query)
            for row in query_result:
                results.append({
                    "job": str(row.job).split('#')[-1] if hasattr(row, 'job') and row.job else None,
                    "machine": str(row.machine).split('#')[-1] if hasattr(row, 'machine') and row.machine else None,
                    "startTime": str(row.startTime) if hasattr(row, 'startTime') else None,
                    "status": str(row.status) if hasattr(row, 'status') else None
                })
        except Exception as e:
            print(f"âš ï¸ ì œí’ˆ ì¶”ì  ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _execute_job_template_query(self, dsl_input: Dict) -> List[Dict]:
        """Job í…œí”Œë¦¿ ì¿¼ë¦¬ ì‹¤í–‰"""
        query = self._build_job_template_query(dsl_input)
        
        results = []
        try:
            query_result = self.domain_graph.query(query)
            for row in query_result:
                results.append({
                    "operation": str(row.operation).split('#')[-1] if hasattr(row, 'operation') and row.operation else None,
                    "machine": str(row.machine).split('#')[-1] if hasattr(row, 'machine') and row.machine else None,
                    "duration": float(row.duration) if hasattr(row, 'duration') and row.duration else None
                })
        except Exception as e:
            print(f"âš ï¸ Job í…œí”Œë¦¿ ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _execute_location_query(self, dsl_input: Dict) -> List[Dict]:
        """ìœ„ì¹˜ ì¿¼ë¦¬ ì‹¤í–‰"""
        query = self._build_location_query(dsl_input)
        
        results = []
        try:
            query_result = self.domain_graph.query(query)
            for row in query_result:
                results.append({
                    "job": str(row.job).split('#')[-1] if hasattr(row, 'job') and row.job else None,
                    "status": str(row.status) if hasattr(row, 'status') else None,
                    "machine": str(row.machine).split('#')[-1] if hasattr(row, 'machine') and row.machine else None,
                    "operationIndex": int(row.opIndex) if hasattr(row, 'opIndex') and row.opIndex else None
                })
        except Exception as e:
            print(f"âš ï¸ ìœ„ì¹˜ ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _build_machine_schedule_query(self) -> str:
        """ë¨¸ì‹  ìŠ¤ì¼€ì¤„ ì¡°íšŒ ì¿¼ë¦¬"""
        query = f"""
        PREFIX prod: <{self.PROD}>
        PREFIX rdf: <{RDF}>
        
        SELECT ?machine ?status ?capability
        WHERE {{
            ?machine rdf:type prod:Machine .
            ?machine prod:hasStatus ?status .
            OPTIONAL {{ ?machine prod:hasCapability ?capability }}
        }}
        """
        
        return query.strip()
    
    def _get_result_type(self, goal_name: str) -> str:
        """Goalì— ë”°ë¥¸ ê²°ê³¼ íƒ€ì… ë°˜í™˜"""
        result_types = {
            "query_failed_jobs_with_cooling": "JobReport",
            "detect_anomaly_for_product": "AnomalyReport",
            "predict_first_completion_time": "CompletionTimePrediction",
            "track_product_position": "ProductLocationReport"
        }
        return result_types.get(goal_name, "UnknownReport")
    
    def _load_action_metadata(self):
        """ì˜¨í†¨ë¡œì§€ì—ì„œ Action ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        query = """
        PREFIX exec: <http://example.org/execution-ontology#>
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
        
        SELECT ?action ?outputName ?endpointTemplate ?queryTemplate 
            ?dockerImage ?inputMapping ?enrichmentType
        WHERE {
            ?action rdf:type ?actionClass .
            ?actionClass rdfs:subClassOf* exec:Action .
            OPTIONAL { ?action exec:hasOutputName ?outputName }
            OPTIONAL { ?action exec:hasEndpointTemplate ?endpointTemplate }
            OPTIONAL { ?action exec:hasQueryTemplate ?queryTemplate }
            OPTIONAL { ?action exec:hasDockerImage ?dockerImage }
            OPTIONAL { ?action exec:hasInputMapping ?inputMapping }
            OPTIONAL { ?action exec:hasEnrichmentType ?enrichmentType }
        }
        """
        
        self.action_metadata = {}
        results = self.exec_graph.query(query)
        
        for row in results:
            action_name = str(row.action).split('#')[-1]
            self.action_metadata[action_name] = {
                'outputName': str(row.outputName) if row.outputName else None,
                'endpointTemplate': str(row.endpointTemplate) if row.endpointTemplate else None,
                'queryTemplate': str(row.queryTemplate) if row.queryTemplate else None,
                'dockerImage': str(row.dockerImage) if row.dockerImage else None,
                'inputMapping': str(row.inputMapping) if row.inputMapping else None,
                'enrichmentType': str(row.enrichmentType) if row.enrichmentType else None
            }

    def _process_query_template(self, template: str, parameter_mappings: Dict, dsl_input: Dict) -> str:
        """ì¿¼ë¦¬ í…œí”Œë¦¿ ì²˜ë¦¬"""
        query = template
        
        # %%FILTERS%% ì²˜ë¦¬
        if "%%FILTERS%%" in query:
            filters = []
            if parameter_mappings.get("requiresCooling"):
                filters.append("?job prod:requiresCooling true")
            if parameter_mappings.get("hasStatus"):
                filters.append(f'?job prod:hasStatus "{parameter_mappings["hasStatus"]}"')
            if parameter_mappings.get("dateFilter"):
                filters.append(f'FILTER(STRSTARTS(STR(?startTime), "{parameter_mappings["dateFilter"]}"))')
            
            query = query.replace("%%FILTERS%%", " . ".join(filters))
        
        # %%PRODUCT_ID%% ì²˜ë¦¬
        if "%%PRODUCT_ID%%" in query:
            product_id = dsl_input.get("product_id", "Unknown")
            query = query.replace("%%PRODUCT_ID%%", product_id)
        
        # %%DATE_FILTER%% ì²˜ë¦¬
        if "%%DATE_FILTER%%" in query:
            date_range = dsl_input.get("date_range", {})
            if date_range:
                from_date = f"{date_range.get('from')}T00:00:00"
                to_date = f"{date_range.get('to')}T23:59:59"
                date_filter = f"""
                FILTER(?startTime >= "{from_date}"^^xsd:dateTime && 
                    ?startTime <= "{to_date}"^^xsd:dateTime)
                """
                query = query.replace("%%DATE_FILTER%%", date_filter)
            else:
                query = query.replace("%%DATE_FILTER%%", "")
        
        # %%QUANTITY%% ì²˜ë¦¬
        if "%%QUANTITY%%" in query:
            quantity = dsl_input.get("quantity", 1)
            query = query.replace("%%QUANTITY%%", str(quantity))
        
        return query.strip()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸš€ DSL to Execution Plan - TTL íŒŒì¼ ë¡œë“œ ë²„ì „")
    print("=" * 80)
    
    # í”Œë˜ë„ˆ ì´ˆê¸°í™”
    try:
        planner = OntologyBasedExecutionPlanner()
    except Exception as e:
        print(f"\nâŒ ì˜¨í†¨ë¡œì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("\ní•„ìš”í•œ TTL íŒŒì¼ (ì‹¤í–‰ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìœ„ì¹˜):")
        print("  - execution-ontology.ttl")
        print("  - domain-ontology.ttl")
        print("  - bridge-ontology.ttl")
        print("\nì„ íƒì  íŒŒì¼:")
        print("  - test_data.json")
        return
    
    # í…ŒìŠ¤íŠ¸í•  DSL ì…ë ¥ë“¤
    test_cases = [
        {
            "goal": "query_failed_jobs_with_cooling",
            "date": "2025-07-17"
        },
        {
            "goal": "detect_anomaly_for_product",
            "product_id": "Product-A1",
            "date_range": {
                "from": "2025-07-15",
                "to": "2025-07-17"
            },
            "target_machine": "CoolingMachine-01"
        },
        {
            "goal": "predict_first_completion_time",
            "product_id": "Product-B2",
            "quantity": 100
        },
        {
            "goal": "track_product_position",
            "product_id": "Product-C1"
        }
    ]
    
    # ëª¨ë“  ì¼€ì´ìŠ¤ ì‹¤í–‰
    for i, dsl_input in enumerate(test_cases):
        print(f"\n{'='*80}")
        print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i+1}: {dsl_input['goal']}")
        print("="*80)
        
        execution_plan = planner.process_dsl(dsl_input)
        
        print("\nğŸ“‹ ìµœì¢… ì‹¤í–‰ ê³„íš:")
        print(json.dumps(execution_plan, indent=2, ensure_ascii=False))
        
        # ì‹¤í–‰ ë‹¨ê³„ë³„ ì„¤ëª…
        if "executionSteps" in execution_plan:
            print("\nğŸ“Š ì‹¤í–‰ ë‹¨ê³„ ìƒì„¸:")
            for step in execution_plan["executionSteps"]:
                print(f"\n[Step {step['stepId']}] {step['action']}")
                print(f"  Type: {step['type']}")
                
                if 'generatedQuery' in step:
                    print(f"  Generated Query:")
                    print(f"    {step['generatedQuery'][:200]}..." if len(step['generatedQuery']) > 200 else f"    {step['generatedQuery']}")
                
                if 'expectedResultCount' in step:
                    print(f"  Expected Results: {step['expectedResultCount']} items")
                
                if 'sampleResults' in step:
                    print(f"  Sample Results:")
                    for result in step['sampleResults']:
                        print(f"    - {result}")
                
                if 'model' in step:
                    print(f"  Model: {step['model']}")
                
                if 'endpoint' in step:
                    print(f"  Endpoint: {step['endpoint']}")

if __name__ == "__main__":
    main()