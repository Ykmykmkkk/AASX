# CLAUDE.md - AAS Integration v6: ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œìŠ¤í…œ

## ğŸ¯ í”„ë¡œì íŠ¸ ë¹„ì „: "ë‚˜ì¹¨ë°˜" ì‹œìŠ¤í…œ

### í•µì‹¬ ì»¨ì…‰
ì˜¨í†¨ë¡œì§€ëŠ” ì „ì²´ ì‹œìŠ¤í…œì˜ **"ë‚˜ì¹¨ë°˜"**ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì˜ë„ë¶€í„° ìµœì¢… ê²°ê³¼ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ì•ˆë‚´í•˜ê³  ì§€íœ˜í•©ë‹ˆë‹¤.

```
ì‚¬ìš©ì ì˜ë„ â†’ [ì˜¨í†¨ë¡œì§€ ë‚˜ì¹¨ë°˜] â†’ ì‹¤í–‰ ê³„íš â†’ ë°ì´í„° ìˆ˜ì§‘ â†’ ì²˜ë¦¬ â†’ ê²°ê³¼
                     â†‘
              "ë¬´ì—‡ì„, ì–´ë””ì„œ, ì–´ë–»ê²Œ"
```

### ë‚˜ì¹¨ë°˜ì˜ ì—­í• 
1. **ì˜ë„ í•´ì„**: ì‚¬ìš©ì DSL ì…ë ¥ì„ ì´í•´í•˜ê³  ëª©í‘œ(Goal) ë§¤í•‘
2. **ê²½ë¡œ ì•ˆë‚´**: ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
3. **ìì› íŒŒì•…**: í•„ìš”í•œ ë°ì´í„°ê°€ ì–´ë””ì— ìˆëŠ”ì§€ íŒŒì•…
4. **ë°©ë²• ì œì‹œ**: ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ê°€ì ¸ì˜¤ê³  ì²˜ë¦¬í• ì§€ ê²°ì •
5. **ëŒ€ì•ˆ ì œê³µ**: ì‹¤íŒ¨ ì‹œ í´ë°± ì „ëµ ì œì‹œ
6. **í†µí•© ì§€íœ˜**: ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ê²°ê³¼ ìƒì„±

---

## ğŸ“Š ì œì¡° ë„ë©”ì¸ ë°ì´í„° ê³„ì¸µ êµ¬ì¡°

### 1. Scenario (ìƒì‚° ì‹œë‚˜ë¦¬ì˜¤)
**ì •ì˜**: íŠ¹ì • ì œí’ˆì„ ëª©í‘œ ìˆ˜ëŸ‰ë§Œí¼ ìƒì‚°í•˜ê¸° ìœ„í•œ ì „ì²´ ê³„íš

```yaml
Scenario:
  id: "SCENARIO-2025-001"
  description: "Widget 1000ê°œ ìƒì‚°"
  product: "PART-N-WIDGET"
  target_quantity: 1000
  deadline: "2025-08-01T00:00:00"
  priority: "HIGH"
  jobs: [JOB-001, JOB-002, ..., JOB-100]
  constraints:
    - max_parallel_jobs: 5
    - required_quality_level: 0.99
  estimated_completion: "2025-07-28T15:30:00"
```

### 2. Part (ì œí’ˆ/ë¶€í’ˆ)
**ì •ì˜**: ìƒì‚°ë˜ëŠ” ì œí’ˆ ë˜ëŠ” ë¶€í’ˆì˜ ì‚¬ì–‘ ë° ìš”êµ¬ì‚¬í•­

```yaml
Part:
  id: "PART-N-WIDGET"
  name: "Advanced Widget Model N"
  type: "ASSEMBLY"
  version: "3.2.1"
  
  bill_of_materials:
    - component: "COMP-A-FRAME"
      quantity: 1
    - component: "COMP-B-CIRCUIT"
      quantity: 2
    - component: "COMP-C-SENSOR"
      quantity: 4
  
  manufacturing_requirements:
    requires_cooling: true
    temperature_range:
      min: 15
      max: 25
      unit: "celsius"
    humidity_range:
      min: 30
      max: 60
      unit: "percent"
    clean_room_level: "ISO-7"
  
  quality_specifications:
    tolerance: 0.01
    inspection_points: ["dimensional", "electrical", "thermal"]
    certification_required: ["CE", "UL"]
  
  operations_sequence:
    - "OP-CUTTING"
    - "OP-COOLING"
    - "OP-ASSEMBLY"
    - "OP-TESTING"
    - "OP-PACKAGING"
```

### 3. Job (ì‘ì—…)
**ì •ì˜**: í•˜ë‚˜ì˜ ì œí’ˆì„ ìƒì‚°í•˜ê¸° ìœ„í•œ ì‘ì—… ë‹¨ìœ„

```yaml
Job:
  id: "JOB-001"
  scenario: "SCENARIO-2025-001"
  product: "PART-N-WIDGET"
  batch_size: 10
  priority: "NORMAL"
  
  status: "IN_PROGRESS"
  progress: 60  # percent
  
  timeline:
    scheduled_start: "2025-07-17T08:00:00"
    actual_start: "2025-07-17T08:15:00"
    estimated_end: "2025-07-17T16:00:00"
    actual_end: null
  
  operations:
    - operation_instance: "JOB-001-OP-001"
      operation_type: "OP-CUTTING"
      status: "COMPLETED"
    - operation_instance: "JOB-001-OP-002"
      operation_type: "OP-COOLING"
      status: "FAILED"
    - operation_instance: "JOB-001-OP-003"
      operation_type: "OP-ASSEMBLY"
      status: "PENDING"
  
  assigned_resources:
    machines: ["CNC001", "COOL001", "ASM001"]
    operators: ["OP-JOHN", "OP-JANE"]
    tools: ["TOOL-SET-A"]
  
  quality_metrics:
    defect_rate: 0.02
    rework_count: 1
    inspection_passed: false
  
  failure_log:
    - timestamp: "2025-07-17T10:30:00"
      operation: "OP-COOLING"
      reason: "cooling_system_error"
      error_code: "ERR-COOL-001"
      recovery_action: "manual_intervention"
```

### 4. Operation (ê³µì •)
**ì •ì˜**: Job ë‚´ì˜ ê°œë³„ ì‘ì—… ë‹¨ê³„

```yaml
Operation:
  id: "JOB-001-OP-002"
  type: "OP-COOLING"
  job: "JOB-001"
  sequence: 2
  
  machine_assignment:
    required_capability: "COOLING"
    assigned_machine: "COOL001"
    alternative_machines: ["COOL002", "COOL003"]
  
  parameters:
    target_temperature: 20
    cooling_rate: 5  # degrees/minute
    hold_time: 30  # minutes
    coolant_type: "WATER"
    flow_rate: 10  # liters/minute
  
  execution:
    status: "FAILED"
    start_time: "2025-07-17T10:00:00"
    end_time: "2025-07-17T10:30:00"
    duration: 30  # minutes
    
  sensor_data:
    temperature_readings: [25, 24, 23, 22, 21, 20, 19, 18, 25, 30]  # spike at end
    pressure_readings: [1.0, 1.0, 1.0, 0.9, 0.8, 0.5, 0.3, 0.1, 0.0, 0.0]
    flow_rate_readings: [10, 10, 10, 9, 8, 5, 2, 0, 0, 0]
  
  failure_details:
    failure_mode: "coolant_pump_failure"
    root_cause: "pump_bearing_wear"
    detection_method: "pressure_sensor_alert"
    impact: "temperature_overshoot"
    corrective_action: "pump_replacement"
```

### 5. Machine (ê¸°ê³„/ì¥ë¹„)
**ì •ì˜**: ìƒì‚°ì— ì‚¬ìš©ë˜ëŠ” ê¸°ê³„ ë° ì¥ë¹„

```yaml
Machine:
  id: "CNC001"
  name: "DMG MORI DMU 50"
  type: "CNC_MILLING"
  
  capabilities:
    - "CUTTING"
    - "DRILLING"
    - "MILLING"
  
  specifications:
    power: 15.5  # kW
    weight: 3500  # kg
    workspace:
      x: 500  # mm
      y: 450  # mm
      z: 400  # mm
    spindle_speed:
      min: 20
      max: 12000  # rpm
    tool_capacity: 20
  
  operational_requirements:
    requires_cooling: true
    power_supply: "3-phase-400V"
    compressed_air: true
    maintenance_interval: 500  # hours
  
  current_status:
    state: "IDLE"
    health: 85  # percent
    utilization: 0.75  # last 24 hours
    last_maintenance: "2025-07-01"
    next_maintenance: "2025-08-01"
    total_operating_hours: 4523
  
  current_job: null
  job_queue: []
  
  performance_metrics:
    mtbf: 720  # hours (Mean Time Between Failures)
    mttr: 2    # hours (Mean Time To Repair)
    oee: 0.85  # Overall Equipment Effectiveness
    quality_rate: 0.98
    performance_rate: 0.92
    availability_rate: 0.94
```

---

## ğŸ—ºï¸ ì˜¨í†¨ë¡œì§€ êµ¬ì¡°: 4ê°œ ë ˆì´ì–´

### Layer 1: Domain Ontology (ë„ë©”ì¸ ì˜¨í†¨ë¡œì§€)
**ì—­í• **: ì œì¡° ë„ë©”ì¸ì˜ ê°œë…ê³¼ ê´€ê³„ ì •ì˜ (WHAT)

```turtle
# domain-ontology.ttl
prod:Scenario
    prod:hasProduct prod:Part ;
    prod:generatesJobs prod:Job ;
    prod:hasDeadline xsd:dateTime ;
    prod:hasTargetQuantity xsd:integer .

prod:Part
    prod:hasBOM prod:Component ;
    prod:requiresOperations prod:OperationType ;
    prod:hasManufacturingRequirements prod:Requirement .

prod:Job
    prod:belongsToScenario prod:Scenario ;
    prod:producesProduct prod:Part ;
    prod:consistsOfOperations prod:Operation ;
    prod:executedOnMachines prod:Machine .

prod:Operation
    prod:hasSequence xsd:integer ;
    prod:requiresCapability prod:Capability ;
    prod:hasParameters prod:OperationParameter ;
    prod:generatesData prod:SensorData .

prod:Machine
    prod:providesCapability prod:Capability ;
    prod:hasOperationalStatus prod:Status ;
    prod:executesOperations prod:Operation .
```

### Layer 2: Execution Ontology (ì‹¤í–‰ ì˜¨í†¨ë¡œì§€)
**ì—­í• **: ì‹¤í–‰ ë°©ë²•ê³¼ ì•¡ì…˜ ì •ì˜ (HOW)

```turtle
# execution-ontology.ttl
exec:Goal
    exec:requiresActions exec:Action ;
    exec:hasExecutionPlan exec:ExecutionPlan ;
    exec:producesResult exec:Result .

exec:Action
    exec:hasType exec:ActionType ;
    exec:requiresInput exec:DataRequirement ;
    exec:hasExecutionOrder xsd:integer ;
    exec:hasTimeout xsd:duration .

exec:ActionType
    owl:oneOf (
        exec:DataQuery      # ë°ì´í„° ì¡°íšŒ
        exec:DataTransform  # ë°ì´í„° ë³€í™˜
        exec:ExternalCall   # ì™¸ë¶€ ì„œë¹„ìŠ¤ í˜¸ì¶œ
        exec:Computation    # ê³„ì‚°/ì²˜ë¦¬
        exec:Integration    # ë°ì´í„° í†µí•©
    ) .

exec:ExecutionPlan
    exec:hasSteps exec:ExecutionStep ;
    exec:hasDataFlow exec:DataFlow ;
    exec:hasErrorHandling exec:ErrorStrategy .
```

### Layer 3: Data Source Ontology (ë°ì´í„° ì†ŒìŠ¤ ì˜¨í†¨ë¡œì§€)
**ì—­í• **: ë°ì´í„° ìœ„ì¹˜ì™€ ì ‘ê·¼ ë°©ë²• ì •ì˜ (WHERE)

```turtle
# data-source-ontology.ttl
ds:DataSource
    ds:providesDataType prod:DataType ;
    ds:hasAccessMethod ds:AccessMethod ;
    ds:hasEndpoint xsd:string ;
    ds:hasFallback ds:DataSource .

ds:AccessMethod
    owl:oneOf (
        ds:REST_API
        ds:SPARQL_QUERY
        ds:FILE_READ
        ds:CONTAINER_EXEC
        ds:STREAMING
    ) .

ds:DataMapping
    ds:fromSource ds:DataSource ;
    ds:toTarget exec:DataRequirement ;
    ds:hasTransformation ds:TransformationRule .
```

### Layer 4: Bridge Ontology (ë¸Œë¦¬ì§€ ì˜¨í†¨ë¡œì§€)
**ì—­í• **: DSLê³¼ ì‹¤í–‰ì„ ì—°ê²° (MAPPING)

```turtle
# bridge-ontology.ttl
bridge:DSLGoal
    bridge:mapsToExecutionGoal exec:Goal ;
    bridge:hasParameterMapping bridge:ParameterMap ;
    bridge:hasResultMapping bridge:ResultMap .

bridge:ParameterMap
    bridge:fromDSLParam bridge:DSLParameter ;
    bridge:toExecParam exec:Parameter ;
    bridge:hasTransformation bridge:Transform .
```

---

## ğŸ”„ ì‹¤í–‰ ê³„íš ìƒì„± í”„ë¡œì„¸ìŠ¤

### Phase 1: ì˜ë„ ë¶„ì„ (Intent Analysis)
```python
DSL Input: {
    "goal": "query_failed_jobs_with_cooling",
    "parameters": {
        "date": "2025-07-17",
        "product_filter": "requires_cooling"
    }
}
    â†“
ì˜¨í†¨ë¡œì§€ ì¿¼ë¦¬:
    "ì´ goalì€ ì–´ë–¤ ì‹¤í–‰ ëª©í‘œì™€ ë§¤í•‘ë˜ëŠ”ê°€?"
    â†“
Execution Goal: exec:QueryFailedCoolingJobs
```

### Phase 2: ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ (Planning)
```python
ì˜¨í†¨ë¡œì§€ ì¿¼ë¦¬:
    "ì´ Goalì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ Actionë“¤ì€?"
    â†“
Actions:
    1. exec:IdentifyCoolingProducts (SPARQL)
    2. exec:GetMachineCapabilities (AAS API)
    3. exec:QueryJobHistory (SPARQL + AAS)
    4. exec:FilterFailedJobs (In-memory)
    5. exec:AnalyzeFailurePatterns (Docker AI)
    6. exec:GenerateReport (Integration)
```

### Phase 3: ë°ì´í„° ìš”êµ¬ì‚¬í•­ íŒŒì•… (Data Requirements)
```python
ê° Actionë³„ í•„ìš” ë°ì´í„°:
    Action 1: Product specifications (cooling requirements)
    Action 2: Machine capabilities and status
    Action 3: Job execution history
    Action 4: Failure criteria
    Action 5: Sensor data for AI analysis
    Action 6: All previous results
```

### Phase 4: ë°ì´í„° ì†ŒìŠ¤ ê²°ì • (Source Selection)
```python
ì˜¨í†¨ë¡œì§€ ì¿¼ë¦¬:
    "ê° ë°ì´í„°ëŠ” ì–´ë””ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ”ê°€?"
    â†“
Data Sources:
    Product specs: SPARQL (primary) â†’ Local TTL (fallback)
    Machine data: AAS API (primary) â†’ Mock Server (fallback)
    Job history: AAS Submodel â†’ SPARQL â†’ Local cache
    Sensor data: S3 TimeSeries â†’ AAS API â†’ Local files
```

### Phase 5: ì‹¤í–‰ íë¦„ êµ¬ì„± (Execution Flow)
```yaml
ExecutionPlan:
  goal: "QueryFailedCoolingJobs"
  total_steps: 6
  estimated_duration: "PT5M"  # 5 minutes
  
  steps:
    - step: 1
      action: "IdentifyCoolingProducts"
      method: "SPARQL"
      source: "LocalTTL"  # Fuseki not available, using fallback
      query: |
        SELECT ?product ?cooling WHERE {
          ?product a prod:Part ;
                   prod:requiresCooling ?cooling .
          FILTER(?cooling = true)
        }
      output: "cooling_products_list"
      
    - step: 2
      action: "GetMachineCapabilities"
      method: "REST_API"
      source: "MockAASServer"
      endpoint: "/api/machines/cooling-required"
      output: "cooling_machines_list"
      
    - step: 3
      action: "QueryJobHistory"
      method: "PARALLEL"  # ë³‘ë ¬ ì‹¤í–‰
      sources:
        - type: "REST_API"
          endpoint: "/shells/{machine_id}/submodels/JobHistory"
        - type: "SPARQL"
          query: "SELECT ?job WHERE { ?job prod:hasDate '2025-07-17' }"
      output: "all_jobs_data"
      
    - step: 4
      action: "FilterFailedJobs"
      method: "IN_MEMORY"
      input: ["cooling_products_list", "cooling_machines_list", "all_jobs_data"]
      filter_criteria:
        - "job.status == 'FAILED'"
        - "job.product IN cooling_products_list"
        - "job.machine IN cooling_machines_list"
      output: "failed_cooling_jobs"
      
    - step: 5
      action: "AnalyzeFailurePatterns"
      method: "DOCKER"
      container: "anomaly-detector:latest"
      input: 
        jobs: "failed_cooling_jobs"
        sensor_data: "FROM_S3"
      model: "failure_pattern_lstm"
      output: "failure_analysis"
      
    - step: 6
      action: "GenerateReport"
      method: "INTEGRATION"
      input: ["failed_cooling_jobs", "failure_analysis"]
      format: "structured_json"
      output: "final_report"
```

---

## ğŸ”Œ ë‹¤ì¤‘ ë°ì´í„° ì†ŒìŠ¤ í†µí•© ì „ëµ

### 1. ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ (Parallel Collection)
```python
# ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë™ì‹œì— ë°ì´í„° ìˆ˜ì§‘
async def collect_data_parallel():
    tasks = [
        aas_client.get_machines(),      # AAS API
        sparql_client.query_products(), # SPARQL
        s3_client.get_sensor_data(),    # S3
        local_db.get_job_history()      # Local
    ]
    results = await asyncio.gather(*tasks)
    return integrate_results(results)
```

### 2. ê³„ì¸µì  í´ë°± (Hierarchical Fallback)
```python
# ê° ë°ì´í„° ì†ŒìŠ¤ë³„ ë…ë¦½ì  í´ë°±
DataSourceHierarchy:
  MachineData:
    1. Live AAS Server (http://production-aas:8080)
    2. Mock AAS Server (http://localhost:5001)
    3. Local JSON Cache (./cache/machines.json)
    4. Static Default Data
    
  JobHistory:
    1. SPARQL Endpoint (http://fuseki:3030)
    2. AAS Submodel API
    3. Local TTL Files
    4. Emergency CSV Backup
```

### 3. ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ (Consistency)
```yaml
ConsistencyRules:
  - TimestampAlignment: "ëª¨ë“  ë°ì´í„°ë¥¼ ë™ì¼ ì‹œì ìœ¼ë¡œ ì •ë ¬"
  - VersionMatching: "ìŠ¤í‚¤ë§ˆ ë²„ì „ í˜¸í™˜ì„± ê²€ì¦"
  - ReferenceIntegrity: "ID ì°¸ì¡° ë¬´ê²°ì„± í™•ì¸"
  - ConflictResolution: "ì¶©ëŒ ì‹œ ìš°ì„ ìˆœìœ„ ê·œì¹™ ì ìš©"
```

---

## ğŸ³ ì™¸ë¶€ ì‹¤í–‰ í™˜ê²½ ì •ì˜

### 1. Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰
```yaml
Containers:
  ProductionSimulator:
    image: "production-simulator:latest"
    purpose: "ì œì¡° ê³µì • ì‹œë®¬ë ˆì´ì…˜"
    input:
      - scenario_definition
      - machine_status
      - job_queue
    output:
      - execution_timeline
      - resource_utilization
      - bottleneck_analysis
    resources:
      cpu: 2
      memory: "4Gi"
      timeout: "300s"
  
  AnomalyDetector:
    image: "anomaly-detector:latest"
    purpose: "ì´ìƒ íŒ¨í„´ ê°ì§€"
    input:
      - sensor_timeseries
      - job_history
      - machine_logs
    output:
      - anomaly_scores
      - root_cause_analysis
      - predictive_alerts
    model:
      type: "LSTM_Autoencoder"
      version: "2.3.1"
      accuracy: 0.94
  
  CompletionPredictor:
    image: "completion-predictor:latest"
    purpose: "ì™„ë£Œ ì‹œê°„ ì˜ˆì¸¡"
    input:
      - job_template
      - machine_schedule
      - historical_performance
    output:
      - predicted_completion_time
      - confidence_interval
      - risk_factors
```

### 2. AWS Lambda í•¨ìˆ˜
```yaml
LambdaFunctions:
  QuickCalculation:
    arn: "arn:aws:lambda:us-east-1:123456789:function:quick-calc"
    purpose: "ê°„ë‹¨í•œ ê³„ì‚° ì‘ì—…"
    max_duration: "15s"
    
  DataTransformation:
    arn: "arn:aws:lambda:us-east-1:123456789:function:data-transform"
    purpose: "ë°ì´í„° í˜•ì‹ ë³€í™˜"
    max_duration: "60s"
```

### 3. Kubernetes Jobs
```yaml
K8sJobs:
  BatchProcessing:
    namespace: "production"
    job_template: "batch-processor"
    purpose: "ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬"
    parallelism: 10
    completions: 100
```

---

## ğŸ® ì‹œìŠ¤í…œ í†µí•© ì•„í‚¤í…ì²˜

### ì „ì²´ íë¦„ë„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì‚¬ìš©ì DSL  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ì˜¨í†¨ë¡œì§€ ë‚˜ì¹¨ë°˜                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ì˜ë„ ë¶„ì„  â”‚â†’â”‚ì‹¤í–‰ ê³„íš  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â†“              â†“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ë°ì´í„°ìš”êµ¬ â”‚â†’â”‚ì†ŒìŠ¤ ê²°ì •  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“              â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚AAS API â”‚    â”‚ SPARQL â”‚    â”‚   S3   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“              â†“              â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ë°ì´í„° í†µí•©     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ì™¸ë¶€ ì²˜ë¦¬       â”‚
         â”‚ (Docker/Lambda) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ìµœì¢… ê²°ê³¼      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸
```python
class OntologyNavigator:
    """ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ë‚˜ì¹¨ë°˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # 4ê°œ ë ˆì´ì–´ ì˜¨í†¨ë¡œì§€
        self.domain_ontology = DomainOntology()
        self.execution_ontology = ExecutionOntology()
        self.datasource_ontology = DataSourceOntology()
        self.bridge_ontology = BridgeOntology()
        
        # ì‹¤í–‰ ì»´í¬ë„ŒíŠ¸
        self.planner = ExecutionPlanner()
        self.data_collector = MultiSourceCollector()
        self.external_executor = ExternalExecutor()
        self.result_integrator = ResultIntegrator()
    
    def navigate(self, user_intent):
        """ì‚¬ìš©ì ì˜ë„ë¶€í„° ê²°ê³¼ê¹Œì§€ ì•ˆë‚´"""
        # 1. ì˜ë„ â†’ Goal
        goal = self.bridge_ontology.map_intent_to_goal(user_intent)
        
        # 2. Goal â†’ Plan
        plan = self.execution_ontology.generate_plan(goal)
        
        # 3. Plan â†’ Data Collection
        data = self.datasource_ontology.collect_required_data(plan)
        
        # 4. Data â†’ Processing
        processed = self.external_executor.process(data, plan)
        
        # 5. Processing â†’ Result
        result = self.result_integrator.integrate(processed)
        
        return result
```

---

## ğŸ“ˆ ì„±ê³µ ì§€í‘œ

### ê¸°ìˆ ì  ì§€í‘œ
- **ì‘ë‹µ ì‹œê°„**: < 5ì´ˆ (95 percentile)
- **ë°ì´í„° ì •í™•ë„**: > 99%
- **ì‹œìŠ¤í…œ ê°€ìš©ì„±**: > 99.9%
- **í´ë°± ì„±ê³µë¥ **: > 95%

### ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ
- **ìƒì‚° íš¨ìœ¨ì„± í–¥ìƒ**: 20%
- **ì¥ì•  ì˜ˆì¸¡ ì •í™•ë„**: 85%
- **ë‹¤ìš´íƒ€ì„ ê°ì†Œ**: 30%
- **í’ˆì§ˆ ê°œì„ **: 15%

---

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ê¸°ë°˜ êµ¬ì¶• (2ì£¼)
- [ ] ì˜¨í†¨ë¡œì§€ íŒŒì¼ ì™„ì„±
- [ ] ë°ì´í„° ëª¨ë¸ ì •ì˜
- [ ] ê¸°ë³¸ ì‹¤í–‰ ì—”ì§„

### Phase 2: í†µí•© êµ¬í˜„ (3ì£¼)
- [ ] ë‹¤ì¤‘ ì†ŒìŠ¤ ì–´ëŒ‘í„°
- [ ] ì‹¤í–‰ ê³„íš ìƒì„±ê¸°
- [ ] í´ë°± ë©”ì»¤ë‹ˆì¦˜

### Phase 3: ì™¸ë¶€ ì—°ë™ (2ì£¼)
- [ ] Docker ì»¨í…Œì´ë„ˆ í†µí•©
- [ ] AI ëª¨ë¸ ì—°ë™
- [ ] ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°

### Phase 4: ìµœì í™” (2ì£¼)
- [ ] ì„±ëŠ¥ íŠœë‹
- [ ] ìºì‹± ì „ëµ
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### Phase 5: ê²€ì¦ (1ì£¼)
- [ ] í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] ë¬¸ì„œí™”

---

## ğŸ“ í•µì‹¬ ì°¨ë³„ì 

### v5ì™€ì˜ ì°¨ì´
1. **ì˜¨í†¨ë¡œì§€ ì¤‘ì‹¬**: í•˜ë“œì½”ë”© ì œê±°, ëª¨ë“  ê²ƒì´ ì˜¨í†¨ë¡œì§€ ê¸°ë°˜
2. **ê³„ì¸µ êµ¬ì¡°**: Scenario â†’ Part â†’ Job â†’ Operation ëª…í™•í•œ ì •ì˜
3. **ë‹¤ì¤‘ ì†ŒìŠ¤**: ë‹¨ì¼ AASê°€ ì•„ë‹Œ ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ í†µí•©
4. **ì™¸ë¶€ ì²˜ë¦¬**: Docker/Lambdaë¥¼ í†µí•œ í™•ì¥ ê°€ëŠ¥í•œ ì²˜ë¦¬
5. **ë‚˜ì¹¨ë°˜ ì—­í• **: ì˜¨í†¨ë¡œì§€ê°€ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì•ˆë‚´

### í•µì‹¬ í˜ì‹ 
- **ë™ì  ì‹¤í–‰**: ì˜¨í†¨ë¡œì§€ ë³€ê²½ë§Œìœ¼ë¡œ ë™ì‘ ë³€ê²½ ê°€ëŠ¥
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ Goal, Action, DataSource ì‰½ê²Œ ì¶”ê°€
- **ë³µì›ë ¥**: ê³„ì¸µì  í´ë°±ìœ¼ë¡œ ë†’ì€ ê°€ìš©ì„±
- **ì§€ëŠ¥í™”**: AI ëª¨ë¸ í†µí•©ìœ¼ë¡œ ì˜ˆì¸¡ ë° ìµœì í™”

---

## ğŸ¯ 4ê°œ Goal ìƒì„¸ êµ¬í˜„ ê³„íš

### Goal 1: ëƒ‰ê° ì‹¤íŒ¨ ì‘ì—… ì¡°íšŒ (query_failed_jobs_with_cooling)

#### ì •ë³´ ìš”êµ¬ì‚¬í•­
```yaml
Required Data:
  1. Cooling Products:
     - What: ëƒ‰ê°ì´ í•„ìš”í•œ ì œí’ˆ ëª©ë¡
     - Where: In-memory SPARQL (products.ttl)
     - How: SPARQL Query
     - Schema: {product_id, name, requires_cooling, temperature_range}
  
  2. Cooling Machines:
     - What: ëƒ‰ê° ê¸°ëŠ¥ì´ ìˆëŠ” ê¸°ê³„ ëª©ë¡
     - Where: Mock AAS Server
     - How: REST API GET /api/machines/cooling-required
     - Schema: {machine_id, name, capability: "COOLING", status}
  
  3. Job History:
     - What: íŠ¹ì • ë‚ ì§œì˜ ì‘ì—… ì´ë ¥
     - Where: Mock AAS Server + In-memory TTL
     - How: GET /shells/{machine}/submodels/JobHistory
     - Schema: {job_id, product_id, machine_id, status, date, failure_reason}

Output Format:
  - failed_jobs: [{job_id, product, machine, failure_reason, timestamp}]
  - summary: {total_failed, cooling_related, date_range}
```

#### ì˜¨í†¨ë¡œì§€ ì•¡ì…˜ ì‹œí€€ìŠ¤
```turtle
exec:Goal1_QueryFailedCoolingJobs
    exec:step1 exec:QueryCoolingProducts ;
    exec:step2 exec:FetchCoolingMachines ;
    exec:step3 exec:CollectJobHistory ;
    exec:step4 exec:FilterFailedJobs ;
    exec:step5 exec:GenerateReport .
```

### Goal 2: ì œí’ˆ ì´ìƒ ê°ì§€ (detect_anomaly_for_product)

#### ì •ë³´ ìš”êµ¬ì‚¬í•­
```yaml
Required Data:
  1. Product Manufacturing History:
     - What: íŠ¹ì • ì œí’ˆì˜ ì œì¡° ì´ë ¥
     - Where: Mock AAS Server
     - How: GET /shells/Product-{id}/submodels/ManufacturingHistory
     - Schema: {product_id, operations: [{timestamp, values}]}
  
  2. Sensor Time Series:
     - What: ì„¼ì„œ ì‹œê³„ì—´ ë°ì´í„°
     - Where: Local Files (ì‹œë®¬ë ˆì´ì…˜)
     - How: File read from ./timeseries/{date}/{product}.json
     - Schema: {timestamp, temperature, pressure, vibration}
  
  3. Normal Pattern Baseline:
     - What: ì •ìƒ íŒ¨í„´ ê¸°ì¤€ê°’
     - Where: In-memory SPARQL
     - How: SPARQL Query
     - Schema: {metric, min, max, mean, std_dev}

Container Execution:
  - Image: anomaly-detector:latest
  - Input: {sensor_data, baseline, threshold: 0.85}
  - Output: {anomaly_score, detected_anomalies: [{timestamp, metric, score}]}
```

#### ì˜¨í†¨ë¡œì§€ ì•¡ì…˜ ì‹œí€€ìŠ¤
```turtle
exec:Goal2_DetectAnomalyForProduct
    exec:step1 exec:FetchProductHistory ;
    exec:step2 exec:LoadSensorData ;
    exec:step3 exec:QueryNormalBaseline ;
    exec:step4 exec:RunAnomalyDetection ;  # Docker container
    exec:step5 exec:InterpretResults .
```

### Goal 3: ì™„ë£Œ ì‹œê°„ ì˜ˆì¸¡ (predict_first_completion_time)

#### ì •ë³´ ìš”êµ¬ì‚¬í•­
```yaml
Required Data:
  1. Job Template:
     - What: ì œí’ˆì˜ í‘œì¤€ ì‘ì—… í…œí”Œë¦¿
     - Where: Mock AAS Server
     - How: GET /shells/Product-{id}/submodels/JobTemplate
     - Schema: {operations: [{name, duration, machine_type, sequence}]}
  
  2. Machine Schedule:
     - What: ê¸°ê³„ ì¼ì • ë° ê°€ìš©ì„±
     - Where: Mock AAS Server
     - How: GET /api/machines/schedule
     - Schema: {machine_id, current_job, queue: [], availability}
  
  3. Historical Performance:
     - What: ê³¼ê±° ìˆ˜í–‰ ì‹œê°„ í†µê³„
     - Where: In-memory SPARQL
     - How: SPARQL Query
     - Schema: {operation, avg_duration, min, max, variance}

Container Execution:
  - Image: production-simulator:latest
  - Input: {job_template, machine_schedule, quantity, constraints}
  - Output: {predicted_completion, confidence, critical_path, bottlenecks}
```

#### ì˜¨í†¨ë¡œì§€ ì•¡ì…˜ ì‹œí€€ìŠ¤
```turtle
exec:Goal3_PredictCompletionTime
    exec:step1 exec:FetchJobTemplate ;
    exec:step2 exec:CheckMachineSchedule ;
    exec:step3 exec:AnalyzeHistoricalData ;
    exec:step4 exec:RunSimulation ;  # Docker container
    exec:step5 exec:CalculatePrediction .
```

### Goal 4: ì œí’ˆ ìœ„ì¹˜ ì¶”ì  (track_product_position)

#### ì •ë³´ ìš”êµ¬ì‚¬í•­
```yaml
Required Data:
  1. Product Shell:
     - What: ì œí’ˆ AAS Shell ì •ë³´
     - Where: Mock AAS Server
     - How: GET /shells/Product-{id}
     - Schema: {product_id, current_status, last_updated}
  
  2. Current Operation:
     - What: í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì‘ì—…
     - Where: Mock AAS Server
     - How: GET /shells/Product-{id}/submodels/CurrentOperation
     - Schema: {operation_id, type, machine_id, progress}
  
  3. Location Info:
     - What: ë¬¼ë¦¬ì  ìœ„ì¹˜ ì •ë³´
     - Where: In-memory SPARQL or AAS
     - How: Query or API call
     - Schema: {location_type, coordinates, zone, machine_id}

Output Format:
  - current_position: {zone, machine, coordinates}
  - status: {operation, progress, estimated_remaining}
  - history: [{timestamp, location, operation}]
```

#### ì˜¨í†¨ë¡œì§€ ì•¡ì…˜ ì‹œí€€ìŠ¤
```turtle
exec:Goal4_TrackProductPosition
    exec:step1 exec:FetchProductShell ;
    exec:step2 exec:GetCurrentOperation ;
    exec:step3 exec:QueryLocationInfo ;
    exec:step4 exec:CompileTrackingData ;
    exec:step5 exec:FormatResponse .
```

---

## ğŸ³ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ëª¨ë“ˆ ì„¤ê³„

### ê¸°ë³¸ êµ¬ì¡°
```python
# container_executor.py

class ContainerExecutor:
    """ì™¸ë¶€ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ëª¨ë“ˆ"""
    
    def __init__(self):
        self.docker_client = None  # Docker SDK client
        self.available_images = {
            'anomaly-detector': 'anomaly-detector:latest',
            'production-simulator': 'production-simulator:latest',
            'completion-predictor': 'completion-predictor:latest'
        }
    
    def execute_container(self, 
                         container_type: str,
                         input_data: Dict[str, Any],
                         timeout: int = 300) -> Dict[str, Any]:
        """
        ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            container_type: ì‹¤í–‰í•  ì»¨í…Œì´ë„ˆ íƒ€ì…
            input_data: ì»¨í…Œì´ë„ˆì— ì „ë‹¬í•  ì…ë ¥ ë°ì´í„°
            timeout: ì‹¤í–‰ ì œí•œ ì‹œê°„ (ì´ˆ)
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        # ì‹¤ì œ êµ¬í˜„ì€ ì£¼ì„ìœ¼ë¡œ ëŒ€ì²´
        """
        1. Docker ì´ë¯¸ì§€ í™•ì¸
        2. ì…ë ¥ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”
        3. ì»¨í…Œì´ë„ˆ ì‹¤í–‰
        4. ê²°ê³¼ ìˆ˜ì§‘
        5. ì •ë¦¬ ë° ë°˜í™˜
        """
        
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì‘ë‹µ
        print(f"ğŸ³ Executing container: {container_type}")
        print(f"ğŸ“¦ Input data keys: {list(input_data.keys())}")
        
        # ì»¨í…Œì´ë„ˆë³„ ë”ë¯¸ ì‘ë‹µ ìƒì„±
        if container_type == 'anomaly-detector':
            return self._mock_anomaly_detection(input_data)
        elif container_type == 'production-simulator':
            return self._mock_simulation(input_data)
        else:
            return {"status": "success", "message": f"Container {container_type} executed"}
    
    def _mock_anomaly_detection(self, input_data: Dict) -> Dict:
        """Goal 2ìš© ëª¨ì˜ ì´ìƒ ê°ì§€ ê²°ê³¼"""
        return {
            "anomaly_score": 0.23,
            "is_anomaly": False,
            "detected_patterns": [
                {"timestamp": "2025-07-17T10:30:00", "metric": "temperature", "score": 0.15},
                {"timestamp": "2025-07-17T11:45:00", "metric": "vibration", "score": 0.31}
            ],
            "confidence": 0.87,
            "message": "No significant anomalies detected"
        }
    
    def _mock_simulation(self, input_data: Dict) -> Dict:
        """Goal 3ìš© ëª¨ì˜ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼"""
        import random
        from datetime import datetime, timedelta
        
        # ì…ë ¥ì—ì„œ ìˆ˜ëŸ‰ ì¶”ì¶œ
        quantity = input_data.get('quantity', 100)
        
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì™„ë£Œ ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
        now = datetime.now()
        hours_per_unit = random.uniform(0.5, 1.5)
        completion_time = now + timedelta(hours=hours_per_unit * quantity)
        
        return {
            "predicted_completion": completion_time.isoformat(),
            "confidence_interval": {
                "lower": (completion_time - timedelta(hours=quantity*0.1)).isoformat(),
                "upper": (completion_time + timedelta(hours=quantity*0.1)).isoformat()
            },
            "confidence": 0.82,
            "critical_path": ["Cutting", "Cooling", "Assembly"],
            "bottlenecks": [
                {"resource": "CNC001", "utilization": 0.95},
                {"resource": "COOL001", "utilization": 0.88}
            ],
            "message": f"Simulation completed for {quantity} units"
        }
```

---

## ğŸ“Š ë°ì´í„° ì†ŒìŠ¤ ì ‘ê·¼ ë°©ë²•

### 1. Mock AAS Server (v5 í™œìš©)
```python
# v5ì˜ ê¸°ì¡´ Mock Server ì‚¬ìš©
# í¬íŠ¸: 5001
# ì—”ë“œí¬ì¸íŠ¸:
#   - /shells: ëª¨ë“  Shell ì¡°íšŒ
#   - /shells/{id}: íŠ¹ì • Shell ì¡°íšŒ
#   - /shells/{id}/submodels: Submodel ëª©ë¡
#   - /api/machines/cooling-required: ëƒ‰ê° ê¸°ê³„ ì¡°íšŒ
#   - /api/snapshot/{timestamp}: íŠ¹ì • ì‹œì  ìŠ¤ëƒ…ìƒ· ì¡°íšŒ (ì¶”ê°€)
```

### 2. In-memory SPARQL
```python
# sparql_engine.py
from rdflib import Graph, Namespace

class InMemorySPARQL:
    def __init__(self):
        self.graph = Graph()
        self.load_test_data()
    
    def load_test_data(self):
        # TTL íŒŒì¼ ë¡œë“œ
        self.graph.parse("./test-data/products.ttl", format="turtle")
        self.graph.parse("./test-data/jobs.ttl", format="turtle")
    
    def query(self, sparql_query: str) -> List[Dict]:
        # SPARQL ì¿¼ë¦¬ ì‹¤í–‰
        results = self.graph.query(sparql_query)
        return [dict(row) for row in results]
```

---

## ğŸ“¸ ìŠ¤ëƒ…ìƒ· ë°ì´í„° ì „ëµ

### ê°œë…
ì‹¤ì œ ê¸°ê¸° ì—°ë™ ì—†ì´ ì‹œê°„ì— ë”°ë¥¸ ìƒíƒœ ë³€í™”ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•œ **ì‹œì ë³„ ìƒíƒœ ì €ì¥** ë°©ì‹

### ìŠ¤ëƒ…ìƒ· ì‹œë‚˜ë¦¬ì˜¤
```yaml
ì‹œë‚˜ë¦¬ì˜¤: "Widget 100ê°œ ìƒì‚° ì¤‘ ëƒ‰ê° ì‹¤íŒ¨ ë°œìƒ"

ìŠ¤ëƒ…ìƒ· ì‹œì :
  T1 (2025-07-17 08:00): ì‘ì—… ì‹œì‘
    - JOB-001: PENDING
    - ëª¨ë“  ê¸°ê³„: IDLE
    - ì„¼ì„œ: ì •ìƒ ë²”ìœ„
  
  T2 (2025-07-17 10:00): ì •ìƒ ì‘ë™
    - JOB-001: IN_PROGRESS (30%)
    - CNC001: RUNNING
    - ì„¼ì„œ: ì˜¨ë„ 22Â°C, ì••ë ¥ 1.0 bar
  
  T3 (2025-07-17 12:00): ì´ìƒ ì§•í›„
    - JOB-001: IN_PROGRESS (60%)
    - COOL001: WARNING
    - ì„¼ì„œ: ì˜¨ë„ 28Â°C (ìƒìŠ¹), ì••ë ¥ 0.8 bar (í•˜ë½)
  
  T4 (2025-07-17 14:00): ëƒ‰ê° ì‹¤íŒ¨
    - JOB-001: FAILED
    - COOL001: ERROR
    - ì„¼ì„œ: ì˜¨ë„ 35Â°C (ê³¼ì—´), ì••ë ¥ 0.3 bar
  
  T5 (2025-07-17 16:00): ë³µêµ¬ í›„ ì¬ì‹œì‘
    - JOB-002: IN_PROGRESS
    - COOL001: MAINTENANCE
    - ì„¼ì„œ: ì •ìƒí™” ì§„í–‰ ì¤‘
```

### ìŠ¤ëƒ…ìƒ· ë°ì´í„° êµ¬ì¡°
```json
// snapshots/2025-07-17T10:00:00.json
{
  "timestamp": "2025-07-17T10:00:00",
  "scenario": "cooling_failure_scenario",
  "jobs": [
    {
      "id": "JOB-001",
      "product": "Product-B1",
      "status": "IN_PROGRESS",
      "progress": 30,
      "machine": "CNC001",
      "operation": "Cutting"
    }
  ],
  "machines": [
    {
      "id": "CNC001",
      "status": "RUNNING",
      "current_job": "JOB-001",
      "health": 95
    },
    {
      "id": "COOL001",
      "status": "IDLE",
      "health": 85
    }
  ],
  "sensor_data": {
    "CNC001": {
      "temperature": 22.5,
      "pressure": 1.01,
      "vibration": 0.5
    }
  }
}
```

### Goalë³„ ìŠ¤ëƒ…ìƒ· í™œìš©

#### Goal 1: ëƒ‰ê° ì‹¤íŒ¨ ì¡°íšŒ
```python
# T4 ì‹œì  ìŠ¤ëƒ…ìƒ·ì—ì„œ ì‹¤íŒ¨ ì‘ì—… ì¡°íšŒ
snapshot = get_snapshot("2025-07-17T14:00:00")
failed_jobs = [j for j in snapshot["jobs"] if j["status"] == "FAILED"]
```

#### Goal 2: ì´ìƒ ê°ì§€
```python
# T2, T3 ì‹œì  ë¹„êµë¡œ ì´ìƒ íŒ¨í„´ ê°ì§€
snapshot_t2 = get_snapshot("2025-07-17T10:00:00")
snapshot_t3 = get_snapshot("2025-07-17T12:00:00")
# ì˜¨ë„ ìƒìŠ¹, ì••ë ¥ í•˜ë½ ì¶”ì„¸ ë¶„ì„
```

#### Goal 3: ì™„ë£Œ ì‹œê°„ ì˜ˆì¸¡
```python
# í˜„ì¬ ì‹œì  ìŠ¤ëƒ…ìƒ· + ê³¼ê±° íŒ¨í„´ìœ¼ë¡œ ì˜ˆì¸¡
current = get_snapshot("2025-07-17T10:00:00")
# ë‚¨ì€ ì‘ì—…ëŸ‰ê³¼ ê¸°ê³„ ê°€ìš©ì„± ê¸°ë°˜ ê³„ì‚°
```

#### Goal 4: ìœ„ì¹˜ ì¶”ì 
```python
# íŠ¹ì • ì‹œì ì˜ ì œí’ˆ ìœ„ì¹˜ í™•ì¸
snapshot = get_snapshot("2025-07-17T12:00:00")
location = snapshot["jobs"][0]["machine"]  # CNC001
```

---

## ğŸ”§ í†µí•© ì‹¤í–‰ ì—”ì§„ êµ¬ì¡°

```python
class OntologyNavigator:
    """ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì‹¤í–‰ ì—”ì§„"""
    
    def __init__(self):
        # ë°ì´í„° ì†ŒìŠ¤
        self.aas_client = AASClient("http://localhost:5001")
        self.sparql = InMemorySPARQL()
        self.container = ContainerExecutor()
        
        # ì˜¨í†¨ë¡œì§€
        self.ontology = OntologyManager()
    
    def execute(self, dsl_input: Dict) -> Dict:
        # 1. Goal ë§¤í•‘
        goal = dsl_input['goal']
        
        # 2. ì˜¨í†¨ë¡œì§€ì—ì„œ ì‹¤í–‰ ê³„íš ì¡°íšŒ
        plan = self.ontology.get_execution_plan(goal)
        
        # 3. ë‹¨ê³„ë³„ ì‹¤í–‰
        results = {}
        for step in plan:
            if step.type == "SPARQL":
                results[step.id] = self.sparql.query(step.query)
            elif step.type == "AAS_API":
                results[step.id] = self.aas_client.get(step.endpoint)
            elif step.type == "CONTAINER":
                results[step.id] = self.container.execute(step.container, step.input)
            elif step.type == "FILTER":
                results[step.id] = self.filter_data(results, step.criteria)
        
        # 4. ê²°ê³¼ í†µí•©
        return self.integrate_results(results)
```

---

## ğŸ“… êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: ê¸°ì´ˆ ì¸í”„ë¼ êµ¬ì¶•
```yaml
ìš°ì„ ìˆœìœ„: HIGH
ëª©í‘œ: ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì‹¤í–‰ ê°€ëŠ¥í•œ ìµœì†Œ ì‹œìŠ¤í…œ

1. ì˜¨í†¨ë¡œì§€ íŒŒì¼ ì‘ì„±:
   - execution-ontology.ttl: 4ê°œ Goalê³¼ Action ì •ì˜
   - domain-ontology.ttl: ì œì¡° ë„ë©”ì¸ ê°œë…
   - bridge-ontology.ttl: DSL ë§¤í•‘
   
2. ìŠ¤ëƒ…ìƒ· ë°ì´í„° ìƒì„±:
   - 5ê°œ ì‹œì  ìŠ¤ëƒ…ìƒ· JSON íŒŒì¼
   - ì œí’ˆ/ê¸°ê³„ ê¸°ë³¸ ë°ì´í„° TTL
   
3. ê¸°ë³¸ ì‹¤í–‰ ì—”ì§„:
   - OntologyManager: ì˜¨í†¨ë¡œì§€ ë¡œë“œ ë° ì¿¼ë¦¬
   - ExecutionPlanner: ì‹¤í–‰ ê³„íš ìƒì„±
   - DataCollector: ë°ì´í„° ìˆ˜ì§‘ ì¸í„°í˜ì´ìŠ¤
```

### Phase 2: Mock Server í™•ì¥
```yaml
ìš°ì„ ìˆœìœ„: HIGH
ëª©í‘œ: ìŠ¤ëƒ…ìƒ· ê¸°ë°˜ ë°ì´í„° ì œê³µ

1. ìŠ¤ëƒ…ìƒ· ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€:
   - GET /api/snapshot/{timestamp}
   - GET /api/snapshots/list
   
2. v5 Mock Server ìˆ˜ì •:
   - ìŠ¤ëƒ…ìƒ· ë°ì´í„° ë¡œë“œ ê¸°ëŠ¥
   - ì‹œì ë³„ ìƒíƒœ ì¡°íšŒ API
```

### Phase 3: Goal 1 êµ¬í˜„ (ê°€ì¥ ë‹¨ìˆœ)
```yaml
ìš°ì„ ìˆœìœ„: HIGH
ëª©í‘œ: ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦

1. SPARQL ì—”ì§„ êµ¬í˜„:
   - ëƒ‰ê° í•„ìš” ì œí’ˆ ì¡°íšŒ
   
2. AAS Client êµ¬í˜„:
   - ëƒ‰ê° ê¸°ê³„ ì¡°íšŒ
   - ì‘ì—… ì´ë ¥ ì¡°íšŒ
   
3. í†µí•© í…ŒìŠ¤íŠ¸:
   - DSL ì…ë ¥ â†’ ì˜¨í†¨ë¡œì§€ â†’ ì‹¤í–‰ â†’ ê²°ê³¼
```

### Phase 4: Goal 4 êµ¬í˜„ (ë‘ ë²ˆì§¸ ë‹¨ìˆœ)
```yaml
ìš°ì„ ìˆœìœ„: MEDIUM
ëª©í‘œ: ìœ„ì¹˜ ì¶”ì  ê¸°ëŠ¥

1. ìŠ¤ëƒ…ìƒ· ê¸°ë°˜ ìœ„ì¹˜ ì¡°íšŒ
2. ì§„í–‰ ìƒíƒœ ê³„ì‚°
3. í†µí•© í…ŒìŠ¤íŠ¸
```

### Phase 5: ì»¨í…Œì´ë„ˆ í†µí•© ì¤€ë¹„
```yaml
ìš°ì„ ìˆœìœ„: MEDIUM
ëª©í‘œ: ì™¸ë¶€ ì‹¤í–‰ í™˜ê²½ êµ¬ì¶•

1. ContainerExecutor êµ¬í˜„
2. í…ŒìŠ¤íŠ¸ìš© Python ì»¨í…Œì´ë„ˆ ìƒì„±
3. Mock ì‘ë‹µ êµ¬í˜„
```

### Phase 6: Goal 2, 3 êµ¬í˜„
```yaml
ìš°ì„ ìˆœìœ„: LOW
ëª©í‘œ: ì»¨í…Œì´ë„ˆ ê¸°ë°˜ ì²˜ë¦¬

1. Goal 2: ì´ìƒ ê°ì§€ (ì»¨í…Œì´ë„ˆ ì‹¤í–‰)
2. Goal 3: ì‹œê°„ ì˜ˆì¸¡ (ì‹œë®¬ë ˆì´í„°)
3. í†µí•© í…ŒìŠ¤íŠ¸
```

---

## ğŸš€ ì¦‰ì‹œ ì‹œì‘í•  ì‘ì—…

### 1. ì˜¨í†¨ë¡œì§€ íŒŒì¼ ìƒì„± (execution-ontology.ttl)
```turtle
@prefix exec: <http://example.org/execution#> .
@prefix prod: <http://example.org/production#> .

# Goal 1 ì •ì˜
exec:QueryFailedCoolingJobs
    a exec:Goal ;
    exec:hasStep exec:Step1_QueryProducts ;
    exec:hasStep exec:Step2_GetMachines ;
    exec:hasStep exec:Step3_GetJobs ;
    exec:hasStep exec:Step4_Filter ;
    exec:hasStep exec:Step5_Report .

exec:Step1_QueryProducts
    a exec:Action ;
    exec:actionType "SPARQL" ;
    exec:query "SELECT ?product WHERE { ?product prod:requiresCooling true }" ;
    exec:order 1 .
```

### 2. ìŠ¤ëƒ…ìƒ· ë°ì´í„° ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
v6/
â”œâ”€â”€ ontology/
â”‚   â”œâ”€â”€ execution-ontology.ttl
â”‚   â”œâ”€â”€ domain-ontology.ttl
â”‚   â””â”€â”€ bridge-ontology.ttl
â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ 2025-07-17T08:00:00.json  # T1: ì‹œì‘
â”‚   â”œâ”€â”€ 2025-07-17T10:00:00.json  # T2: ì •ìƒ
â”‚   â”œâ”€â”€ 2025-07-17T12:00:00.json  # T3: ê²½ê³ 
â”‚   â”œâ”€â”€ 2025-07-17T14:00:00.json  # T4: ì‹¤íŒ¨
â”‚   â””â”€â”€ 2025-07-17T16:00:00.json  # T5: ë³µêµ¬
â”œâ”€â”€ test-data/
â”‚   â”œâ”€â”€ products.ttl
â”‚   â”œâ”€â”€ machines.ttl
â”‚   â””â”€â”€ jobs.ttl
â””â”€â”€ src/
    â”œâ”€â”€ ontology_manager.py
    â”œâ”€â”€ execution_planner.py
    â”œâ”€â”€ container_executor.py
    â””â”€â”€ main.py
```

### 3. ì²« ë²ˆì§¸ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ
```python
# main.py
from ontology_manager import OntologyManager
from execution_planner import ExecutionPlanner

def test_goal1():
    # 1. DSL ì…ë ¥
    dsl_input = {
        "goal": "query_failed_jobs_with_cooling",
        "parameters": {"date": "2025-07-17"}
    }
    
    # 2. ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ê³„íš ìƒì„±
    manager = OntologyManager()
    planner = ExecutionPlanner(manager)
    plan = planner.create_plan(dsl_input)
    
    # 3. ê³„íš ì‹¤í–‰
    result = planner.execute(plan)
    
    print(f"Result: {result}")

if __name__ == "__main__":
    test_goal1()
```

---

**ì‘ì„±ì¼**: 2025-08-06  
**ë²„ì „**: v6 ì´ˆê¸° ì„¤ê³„  
**ìƒíƒœ**: êµ¬í˜„ ìš°ì„ ìˆœìœ„ í™•ì •  
**ë‹¤ìŒ ë‹¨ê³„**: ì˜¨í†¨ë¡œì§€ íŒŒì¼ ì‘ì„± â†’ ìŠ¤ëƒ…ìƒ· ë°ì´í„° ìƒì„± â†’ Goal 1 êµ¬í˜„